#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SPD Phase 1-A: Auto-Fetch Engine
==================================
RFP Radar v8.3 ê²°ê³¼ì—ì„œ S/Aë“±ê¸‰ ê³µê³ ì˜ ì²¨ë¶€íŒŒì¼ì„ ìë™ ë‹¤ìš´ë¡œë“œí•˜ê³ 
HWPâ†’PDF ë³€í™˜ â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œê¹Œì§€ ìˆ˜í–‰í•©ë‹ˆë‹¤.

Usage:
  python spd_auto_fetch.py                        # ìµœìˆ  ë¦¬í¬íŠ¸, S/A ë“±ê¸‰
  python spd_auto_fetch.py --list                 # ëŒ€ìƒ ëª©ë¡ë§ˆ í™•ì¸
  python spd_auto_fetch.py --grade S              # Së“±ê¸‰ë§Œ
  python spd_auto_fetch.py --bid 20260214001      # íŠ¹ì • ê³µê³ 
  python spd_auto_fetch.py --report report_20260214.json  # íŠ¹ì • ë¦¬í¬íŠ¸

Author: WKMG Automation (SPD System)
Version: 1.0.0
"""

import os
import sys
import json
import glob
import time
import hashlib
import argparse
import logging
import subprocess
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, List, Tuple

import requests

try:
    import pdfplumber
except ImportError:
    pdfplumber = None
    print("âš ï¸ pdfplumber ë¯¸ì„¤ì¹˜: pip install pdfplumber")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì„¤ì •
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
CONFIG_PATH = os.path.join(SCRIPT_DIR, "config.json")

DEFAULT_CONFIG = {
    "rfp_radar_data_dir": os.path.join(SCRIPT_DIR, "..", "rfp_radar", "data", "daily_reports"),
    "download_dir": os.path.join(SCRIPT_DIR, "data", "downloads"),
    "extracted_dir": os.path.join(SCRIPT_DIR, "data", "extracted"),
    "output_dir": os.path.join(SCRIPT_DIR, "data", "fetch_results"),
    "libreoffice_path": "",
    "max_file_seq": 20,
    "download_timeout": 30,
    "min_grade": "A",
    "target_sources": ["g2b_api"],
}

def load_config():
    cfg = DEFAULT_CONFIG.copy()
    if os.path.exists(CONFIG_PATH):
        with open(CONFIG_PATH, "r", encoding="utf-8") as f:
            user_cfg = json.load(f)
        if "spd" in user_cfg:
            cfg.update(user_cfg["spd"])
    # í™˜ê²½ë³€ìˆ˜ ìš°ì„ 
    if os.environ.get("RFP_SERVICE_KEY"):
        cfg["service_key"] = os.environ["RFP_SERVICE_KEY"]
    return cfg

# ë¡œê¹…
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
log = logging.getLogger("AutoFetch")

# ë“±ê¸ˆ ìš°ì„ ìˆœìœ„
GRADE_ORDER = {"S": 0, "A": 1, "B": 2, "C": 3}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# G2B ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

G2B_FILE_URL = "https://apis.data.go.kr/1230000/BidPublicInfoService04/getBidPblancListInfoThngFile"

def find_latest_report(data_dir: str) -> Optional[str]:
    """rfp_radar/data/daily_reportsì—ì„œ ìµœì‹  JSON ë¦¬í¬íŠ¸ ì°¾ê¸°"""
    patterns = [
        os.path.join(data_dir, "rfp_report_*.json"),
        os.path.join(data_dir, "report_*.json"),
        os.path.join(data_dir, "*.json"),
    ]
    all_files = []
    for p in patterns:
        all_files.extend(glob.glob(p))
    if not all_files:
        return None
    return max(all_files, key=os.path.getmtime)

def load_report(report_path: str) -> List[Dict]:
    """RFP Radar ë¦¬í¬íŠ¸ì—ì„œ ê³µê³  ëª©ë¡ ë¡œë“œ"""
    with open(report_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # ë‹¤ì–‘í•œ ë¦¬í¬íŠ¸ í˜•ì‹œ ì§€ì›
    if isinstance(data, list):
        return data
    if isinstance(data, dict):
        for key in ["results", "bids", "items", "data"]:
            if key in data and isinstance(data[key], list):
                return data[key]
        # top-level dict with bid data
        if "bid_no" in data or "bidNtceNo" in data:
            return [data]
    return []

def filter_bids(bids: List[Dict], min_grade: str = "A", 
                target_sources: List[str] = None, 
                specific_bid: str = None) -> List[Dict]:
    """ë“±ê¸‰/ì†ŒìŠ¤/ê³µê³ ë²ˆí˜¸ ê¸°ì¤€ í•„í„°ë§"""
    min_grade_idx = GRADE_ORDER.get(min_grade, 1)
    filtered = []
    
    for bid in bids:
        # íŠ¹ì • ê³µê³ ë²ˆí˜¸ ì§€ì •
        if specific_bid:
            bid_no = str(bid.get("bid_no", bid.get("bidNtceNo", "")))
            if specific_bid not in bid_no:
                continue
            filtered.append(bid)
            continue
        
        # ë“±ê¸‰ í•„í„°
        grade = bid.get("grade", bid.get("rfp_grade", "C"))
        if GRADE_ORDER.get(grade, 3) > min_grade_idx:
            continue
        
        # ì†ŒìŠ¤ í•„í„°
        if target_sources:
            source = bid.get("source", bid.get("data_source", ""))
            if source and not any(t in source.lower() for t in target_sources):
                continue
        
        filtered.append(bid)
    
    return filtered

def download_g2b_files(bid_no: str, config: Dict) -> List[Dict]:
    """G2B APIë¡œ ê³µê³  ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    service_key = config.get("service_key", os.environ.get("RFP_SERVICE_KEY", ""))
    if not service_key:
        log.warning(f"  âš ï¸ G2B API í‚¤ ì—†ìŒ â€” ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ìŠ¤í‚µ")
        return []
    
    download_dir = config.get("download_dir", "data/downloads")
    bid_dir = os.path.join(download_dir, str(bid_no))
    os.makedirs(bid_dir, exist_ok=True)
    
    downloaded = []
    max_seq = config.get("max_file_seq", 20)
    timeout = config.get("download_timeout", 30)
    
    for seq in range(1, max_seq + 1):
        params = {
            "ServiceKey": service_key,
            "inqryDiv": "1",
            "bidNtceNo": bid_no,
            "bidNtceOrd": "00",
            "fileSeq": str(seq),
            "type": "json",
        }
        try:
            resp = requests.get(G2B_FILE_URL, params=params, timeout=timeout)
            if resp.status_code != 200:
                continue
            
            data = resp.json()
            items = []
            
            # API ì‘ë‹µ íŒŒì‹±
            body = data.get("response", {}).get("body", {})
            item_list = body.get("items", [])
            if isinstance(item_list, dict):
                item_list = item_list.get("item", [])
            if isinstance(item_list, dict):
                item_list = [item_list]
            items = item_list if isinstance(item_list, list) else []
            
            if not items:
                if seq > 3:  # 3ë²ˆê¸Œì§€ëŠ” ì‹œë„, ì´í›„ ë¹ˆ ì‘ë‹µì´ë©´ ì¤‘ë‹¨
                    break
                continue
            
            for item in items:
                file_url = item.get("fileUrl", item.get("pblancAtchFileUrl", ""))
                file_name = item.get("fileNm", item.get("pblancAtchFileNm", f"file_{seq}"))
                
                if not file_url:
                    continue
                
                # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                file_path = os.path.join(bid_dir, file_name)
                try:
                    file_resp = requests.get(file_url, timeout=timeout)
                    if file_resp.status_code == 200 and len(file_resp.content) > 100:
                        with open(file_path, "wb") as f:
                            f.write(file_resp.content)
                        downloaded.append({
                            "file_name": file_name,
                            "file_path": file_path,
                            "file_size": len(file_resp.content),
                            "file_seq": seq,
                        })
                        log.info(f"    ğŸ“¥ {file_name} ({len(file_resp.content):,}B)")
                except Exception as e:
                    log.warning(f"    âš ï¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {file_name} â€” {e}")
                    
        except Exception as e:
            if seq > 3:
                break
            continue
    
    return downloaded

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# í…ìŠ¤íŠ¸ ì¶”ì¶œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def extract_text_from_pdf(file_path: str) -> str:
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if not pdfplumber:
        return ""
    try:
        texts = []
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages[:50]:  # ìµœëŒ€ 50í˜ì´ì§€
                text = page.extract_text()
                if text:
                    texts.append(text)
        return "\n".join(texts)
    except Exception as e:
        log.warning(f"    âš ï¸ PDF ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""

def convert_hwp_to_pdf(hwp_path: str, config: Dict) -> Optional[str]:
    """HWP â†’ PDF ë³€í™˜ (LibreOffice ì‚¬ìš©)"""
    lo_path = config.get("libreoffice_path", "")
    
    # LibreOffice ê²½ë¡œ ìë™ íƒìƒ‰
    if not lo_path:
        candidates = [
            r"C:\Program Files\LibreOffice\program\soffice.exe",
            r"C:\Program Files (x86)\LibreOffice\program\soffice.exe",
            "/usr/bin/libreoffice",
            "/usr/bin/soffice",
        ]
        for c in candidates:
            if os.path.exists(c):
                lo_path = c
                break
    
    if not lo_path:
        log.warning("    âš ï¸ LibreOffice ë¯¸ì„¤ì¹˜ â€” HWP ë³€í™˜ ë¶ˆê°€")
        return None
    
    output_dir = os.path.dirname(hwp_path)
    try:
        cmd = [lo_path, "--headless", "--convert-to", "pdf", "--outdir", output_dir, hwp_path]
        result = subprocess.run(cmd, capture_output=True, timeout=60)
        
        pdf_path = os.path.splitext(hwp_path)[0] + ".pdf"
        if os.path.exists(pdf_path):
            log.info(f"    ğŸ“„ HWPâ†’PDF ë³€í™˜ ì™„ë£Œ")
            return pdf_path
    except Exception as e:
        log.warning(f"    âš ï¸ HWP ë³€í™˜ ì‹¤íŒ¨: {e}")
    
    return None

def extract_text_from_file(file_path: str, config: Dict) -> str:
    """íŒŒì¼ í˜•ì‹ì— ë”°ë¥¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    ext = os.path.splitext(file_path)[1].lower()
    
    if ext == ".pdf":
        return extract_text_from_pdf(file_path)
    
    elif ext in (".hwp", ".hwpx"):
        pdf_path = convert_hwp_to_pdf(file_path, config)
        if pdf_path:
            return extract_text_from_pdf(pdf_path)
        return ""
    
    elif ext == ".docx":
        try:
            import docx
            doc = docx.Document(file_path)
            return "\n".join(p.text for p in doc.paragraphs if p.text.strip())
        except Exception:
            return ""
    
    elif ext == ".pptx":
        try:
            from pptx import Presentation
            prs = Presentation(file_path)
            texts = []
            for slide in prs.slides:
                for shape in slide.shapes:
                    if shape.has_text_frame:
                        texts.append(shape.text_frame.text)
            return "\n".join(texts)
        except Exception:
            return ""
    
    elif ext == ".txt":
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read()
        except:
            try:
                with open(file_path, "r", encoding="euc-kr") as f:
                    return f.read()
            except:
                return ""
    
    return ""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def process_bid(bid: Dict, config: Dict, dry_run: bool = False) -> Dict:
    """ë‹¨ì¼ ê³µê³  ì²˜ë¦¬: ë‹¤ìš´ë¡œë“œ â†’ ì¶”ì¶œ â†’ ê²°ê³¼ ì €ì¥"""
    bid_no = str(bid.get("bid_no", bid.get("bidNtceNo", "unknown")))
    title = bid.get("title", bid.get("bidNtceNm", ""))
    grade = bid.get("grade", bid.get("rfp_grade", "?"))
    agency = bid.get("agency", bid.get("dminsttNm", ""))
    budget = bid.get("budget_str", bid.get("presmptPrce", ""))
    
    log.info(f"\n{'='*60}")
    log.info(f"ğŸ“‹ [{grade}] {title}")
    log.info(f"   ê³µê³ : {bid_no} | ê¸°ê´€: {agency} | ì˜ˆì‚°: {budget}")
    
    result = {
        "bid_no": bid_no,
        "title": title,
        "grade": grade,
        "agency": agency,
        "budget_str": str(budget),
        "processed_at": datetime.now().isoformat(),
        "files_downloaded": [],
        "texts_extracted": [],
        "total_text_length": 0,
        "status": "pending",
    }
    
    if dry_run:
        result["status"] = "dry_run"
        log.info(f"   ğŸ” DRY-RUN â€” ë‹¤ìš´ë¡œë“œ/ì¶”ì¶œ ìŠ¤í‚µ")
        return result
    
    # Step 1: ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    log.info(f"  ğŸ“¥ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
    downloaded = download_g2b_files(bid_no, config)
    result["files_downloaded"] = downloaded
    
    if not downloaded:
        log.info(f"  âš ï¸ ì²¨ë¶€íŒŒì¼ ì—†ìŒ â€” RFP í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©")
        result["status"] = "no_files"
        return result
    
    # Step 2: í…ìŠ¤íŠ¸ ì¶”ì¶œ
    log.info(f"  ğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘... ({len(downloaded)}ê°œ íŒŒì¼)")
    extracted_dir = config.get("extracted_dir", "data/extracted")
    os.makedirs(extracted_dir, exist_ok=True)
    
    all_texts = []
    for dl in downloaded:
        text = extract_text_from_file(dl["file_path"], config)
        if text:
            # ì¶”ì¶œ í…ìŠ¤íŠ¸ ì €ì¥
            text_file = os.path.join(extracted_dir, f"{bid_no}_{dl['file_seq']}.txt")
            with open(text_file, "w", encoding="utf-8") as f:
                f.write(text)
            
            all_texts.append({
                "file_name": dl["file_name"],
                "text_length": len(text),
                "text_file": text_file,
                "preview": text[:200],
            })
            log.info(f"    âœ… {dl['file_name']} â†’ {len(text):,}ì ì¶”ì¶œ")
    
    result["texts_extracted"] = all_texts
    result["total_text_length"] = sum(t["text_length"] for t in all_texts)
    result["status"] = "completed" if all_texts else "extraction_failed"
    
    log.info(f"  ğŸ“Š ì¶”ì¶œ ì™„ë£Œ: {len(all_texts)}ê°œ íŒŒì¼, ì´ {result['total_text_length']:,}ì")
    
    return result

def run_auto_fetch(args):
    """ë©”ì¸ ì‹¤í–‰"""
    config = load_config()
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    for d in ["download_dir", "extracted_dir", "output_dir"]:
        path = config.get(d, "")
        if path:
            os.makedirs(os.path.join(SCRIPT_DIR, path) if not os.path.isabs(path) else path, exist_ok=True)
    
    # ë¦¬í¬íŠ¸ ë¡œë“œ
    if args.report:
        report_path = args.report
    else:
        data_dir = config.get("rfp_radar_data_dir", "")
        if not os.path.isabs(data_dir):
            data_dir = os.path.join(SCRIPT_DIR, data_dir)
        report_path = find_latest_report(data_dir)
    
    if not report_path or not os.path.exists(report_path):
        log.error(f"âŒ RFP Radar ë¦¬í¬íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        log.info(f"   ê¸°ëŒ€ ê²½ë¡œ: {config.get('rfp_radar_data_dir', '?')}")
        return
    
    log.info(f"ğŸ“‚ ë¦¬í¬íŠ¸: {os.path.basename(report_path)}")
    bids = load_report(report_path)
    log.info(f"   ì „ì²´ ê³µê³ : {len(bids)}ê±´")
    
    # í•„í„°ë§
    min_grade = args.grade or config.get("min_grade", "A")
    target_sources = config.get("target_sources", None)
    filtered = filter_bids(bids, min_grade, target_sources, args.bid)
    
    log.info(f"   ëŒ€ìƒ ê³µê³ : {filtered and len(filtered) or 0}ê±´ (ë“±ê¸‰ {min_grade}+ í•„í„°)")
    
    if not filtered:
        log.info("âœ… ëŒ€ìƒ ê³µê³  ì—†ìŒ. ì¢…ë£Œ.")
        return
    
    # ëª©ë¡ ëª¨ë“œ
    if args.list:
        log.info(f"\n{'='*60}")
        log.info(f"ğŸ“‹ Auto-Fetch ëŒ€ìƒ ëª©ë¡ ({len(filtered)}ê±´)")
        log.info(f"{'='*60}")
        for i, bid in enumerate(filtered, 1):
            grade = bid.get("grade", bid.get("rfp_grade", "?"))
            title = bid.get("title", bid.get("bidNtceNm", ""))[:50]
            bid_no = bid.get("bid_no", bid.get("bidNtceNo", ""))
            log.info(f"  {i:2d}. [{grade}] {title}... ({bid_no})")
        return
    
    # ì²˜ë¦¬ ì‹¤í–‰
    results = []
    for i, bid in enumerate(filtered, 1):
        log.info(f"\n[{i}/{len(filtered)}] ì²˜ë¦¬ ì¤‘...")
        result = process_bid(bid, config, dry_run=args.dry_run)
        results.append(result)
        
        if i < len(filtered):
            time.sleep(1)  # API ë¶€í•˜ ë°©ì§€
    
    # ê²°ê³¼ ì €ì¥
    output_dir = config.get("output_dir", "data/fetch_results")
    if not os.path.isabs(output_dir):
        output_dir = os.path.join(SCRIPT_DIR, output_dir)
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(output_dir, f"fetch_{timestamp}.json")
    
    output_data = {
        "generated_at": datetime.now().isoformat(),
        "report_source": report_path,
        "total_bids": len(bids),
        "filtered_bids": len(filtered),
        "processed": len(results),
        "results": results,
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    # ìš”ì•½
    completed = sum(1 for r in results if r["status"] == "completed")
    total_text = sum(r.get("total_text_length", 0) for r in results)
    
    log.info(f"\n{'='*60}")
    log.info(f"âœ… Auto-Fetch ì™„ë£Œ")
    log.info(f"   ì²˜ë¦¬: {len(results)}ê±´ (ì„±ê³µ: {completed}, íŒŒì¼ì—†ìŒ: {len(results)-completed})")
    log.info(f"   ì¶”ì¶œ í…ìŠ¤íŠ¸: {total_text:,}ì")
    log.info(f"   ê²°ê³¼: {output_file}")
    log.info(f"{'='*60}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="SPD Auto-Fetch Engine")
    parser.add_argument("--report", help="íŠ¹ì • RFP Radar ë¦¬í¨íŠ¸ JSON ê²½ë¡œ")
    parser.add_argument("--grade", choices=["S", "A", "B", "C"], help="ìµœì†Œ ë“±ê¸‰ í•„í„°")
    parser.add_argument("--bid", help="íŠ¹ì • ê³µê³ ë²ˆí˜¸")
    parser.add_argument("--list", action="store_true", help="ëŒ€ìƒ ëª©ë¡ë§Œ í™•ì¸")
    parser.add_argument("--dry-run", action="store_true", help="ë‹¤ìš´ë¡œë“œ/ì¶”ì¶œ ì—†ì´ êµ¬ì¡°ë§Œ í™•ì¸")
    
    args = parser.parse_args()
    run_auto_fetch(args)
